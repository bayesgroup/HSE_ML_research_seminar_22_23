{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[–î–æ–∫–∏](https://guochengqian.github.io/PointNeXt/)"
      ],
      "metadata": {
        "id": "E6buep9d5azM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/guochengqian/PointNeXt\n",
        "cd PointNeXt\n",
        "git clone https://github.com/guochengqian/openpoints\n",
        "pip install torch-scatter, torchvision -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
        "pip install -r requirements.txt\n",
        "cd openpoints/cpp/pointnet2_batch\n",
        "python setup.py install\n",
        "cd ../subsampling\n",
        "python setup.py build_ext --inplace\n",
        "cd ../pointops\n",
        "python setup.py install\n",
        "cd ../chamfer_dist\n",
        "python setup.py install --user\n",
        "cd ../emd\n",
        "python setup.py install --user\n",
        "cd ../../../"
      ],
      "metadata": {
        "id": "WQun1XAUOEJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PointNeXt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjsyIUVARLAe",
        "outputId": "f716438b-e118-4053-a42f-c05054242268"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'PointNeXt'\n",
            "/content/PointNeXt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "id": "iIaFn9EkS6rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p pretrained/imagenet\n",
        "cd pretrained/imagenet\n",
        "gdown https://drive.google.com/file/d/1Iqc-nWVMmm4c8kYshNFcJsthnUy75Jl1/view?usp=sharing --fuzzy\n",
        "mv small_21k_224.pth mae_s.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzQYLxC3KhSb",
        "outputId": "dba772a2-8a47-4c98-d97e-59f8d1c9bbe3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Iqc-nWVMmm4c8kYshNFcJsthnUy75Jl1\n",
            "To: /content/PointNeXt/pretrained/imagenet/small_21k_224.pth\n",
            "\r  0%|          | 0.00/120M [00:00<?, ?B/s]\r 17%|‚ñà‚ñã        | 20.4M/120M [00:00<00:00, 202MB/s]\r 37%|‚ñà‚ñà‚ñà‚ñã      | 44.6M/120M [00:00<00:00, 224MB/s]\r 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 84.4M/120M [00:00<00:00, 302MB/s]\r100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 120M/120M [00:00<00:00, 310MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p data/ShapeNetPart\n",
        "cd data/ShapeNetPart\n",
        "gdown https://drive.google.com/uc?id=1W3SEE-dY1sxvlECcOwWSDYemwHEUbJIS\n",
        "tar -xvf shapenetcore_partanno_segmentation_benchmark_v0_normal.tar"
      ],
      "metadata": {
        "id": "D_YBIuC0U2xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python examples/shapenetpart/main.py --cfg cfgs/shapenetpart_pix4point/pix4point.yaml --wandb.use_wandb True --val_freq 1 --epochs 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISCtwsJeP6PT",
        "outputId": "9c72119d-8bc7-443f-a735-deae98deb0bc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "launch mp with 1 GPUs, current rank: 0\n",
            "\u001b[32m[12/07 11:59:05 ShapeNetPartNormal]: \u001b[0mInitializing MLIR with module: _site_initialize_0\n",
            "\u001b[32m[12/07 11:59:05 ShapeNetPartNormal]: \u001b[0mRegistering dialects from initializer <module 'jaxlib.mlir._mlir_libs._site_initialize_0' from '/usr/local/lib/python3.8/dist-packages/jaxlib/mlir/_mlir_libs/_site_initialize_0.so'>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmakartkar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/PointNeXt/wandb/run-20221207_115907-18vnchmd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mshapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/makartkar/pix4point-ShapeNetPart\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/makartkar/pix4point-ShapeNetPart/runs/18vnchmd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[32m[12/07 11:59:07 ShapeNetPartNormal]: \u001b[0mdist_url: tcp://localhost:8888\n",
            "dist_backend: nccl\n",
            "multiprocessing_distributed: False\n",
            "ngpus_per_node: 1\n",
            "world_size: 1\n",
            "launcher: mp\n",
            "local_rank: 0\n",
            "use_gpu: True\n",
            "seed: 9494\n",
            "epoch: 0\n",
            "epochs: 1\n",
            "ignore_index: None\n",
            "val_fn: validate\n",
            "deterministic: False\n",
            "sync_bn: False\n",
            "criterion_args:\n",
            "  NAME: Poly1FocalLoss\n",
            "use_mask: False\n",
            "grad_norm_clip: 1\n",
            "layer_decay: 0\n",
            "step_per_update: 1\n",
            "start_epoch: 1\n",
            "sched_on_epoch: True\n",
            "wandb:\n",
            "  use_wandb: True\n",
            "  project: pix4point-ShapeNetPart\n",
            "  tags: ['shapenetpart_pix4point', 'finetune_encoder', 'pix4point', 'ngpus1', 'seed9494']\n",
            "  name: shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg\n",
            "use_amp: False\n",
            "use_voting: False\n",
            "val_freq: 1\n",
            "resume: False\n",
            "test: False\n",
            "finetune: False\n",
            "mode: finetune_encoder\n",
            "logname: None\n",
            "load_path: None\n",
            "print_freq: 10\n",
            "save_freq: -1\n",
            "root_dir: log/shapenetpart_pix4point\n",
            "pretrained_path: pretrained/imagenet/mae_s.pth\n",
            "datatransforms:\n",
            "  train: ['PointsToTensor', 'PointCloudScaling', 'PointCloudCenterAndNormalize', 'PointCloudJitter', 'ChromaticDropGPU']\n",
            "  val: ['PointsToTensor', 'PointCloudCenterAndNormalize']\n",
            "  vote: ['PointCloudScaling']\n",
            "  kwargs:\n",
            "    jitter_sigma: 0.001\n",
            "    jitter_clip: 0.005\n",
            "    scale: [0.8, 1.2]\n",
            "    gravity_dim: 1\n",
            "    angle: [0, 1.0, 0]\n",
            "feature_keys: pos,x,heights\n",
            "dataset:\n",
            "  common:\n",
            "    NAME: ShapeNetPartNormal\n",
            "    data_root: data/ShapeNetPart/shapenetcore_partanno_segmentation_benchmark_v0_normal\n",
            "    use_normal: True\n",
            "    num_points: 2048\n",
            "  train:\n",
            "    split: trainval\n",
            "  val:\n",
            "    split: test\n",
            "    presample: True\n",
            "num_classes: 50\n",
            "shape_classes: 16\n",
            "num_points: 2048\n",
            "normal_channel: True\n",
            "batch_size: 8\n",
            "dataloader:\n",
            "  num_workers: 6\n",
            "num_votes: 10\n",
            "refine: True\n",
            "lr: 0.0005\n",
            "optimizer:\n",
            "  NAME: adamw\n",
            "  weight_decay: 0.0001\n",
            "sched: multistep\n",
            "decay_epochs: [210, 270]\n",
            "decay_rate: 0.1\n",
            "warmup_epochs: 0\n",
            "model:\n",
            "  NAME: BasePartSeg\n",
            "  encoder_args:\n",
            "    NAME: PointViT\n",
            "    in_channels: 7\n",
            "    embed_dim: 384\n",
            "    depth: 12\n",
            "    num_heads: 6\n",
            "    mlp_ratio: 4.0\n",
            "    drop_rate: 0.0\n",
            "    attn_drop_rate: 0.0\n",
            "    drop_path_rate: 0.1\n",
            "    add_pos_each_block: True\n",
            "    qkv_bias: True\n",
            "    act_args:\n",
            "      act: gelu\n",
            "    norm_args:\n",
            "      norm: ln\n",
            "      eps: 1e-06\n",
            "    embed_args:\n",
            "      NAME: P3Embed\n",
            "      feature_type: dp_df\n",
            "      reduction: max\n",
            "      sample_ratio: 0.0625\n",
            "      normalize_dp: False\n",
            "      group_size: 32\n",
            "      subsample: fps\n",
            "      group: knn\n",
            "      conv_args:\n",
            "        order: conv-norm-act\n",
            "      layers: 4\n",
            "      norm_args:\n",
            "        norm: bn\n",
            "  decoder_args:\n",
            "    NAME: PointViTPartDecoder\n",
            "    channel_scaling: 1\n",
            "    global_feat: cls,max,avg\n",
            "    progressive_input: True\n",
            "  cls_args:\n",
            "    NAME: SegHead\n",
            "    num_classes: 50\n",
            "    mlps: [256]\n",
            "    in_channels: None\n",
            "    norm_args:\n",
            "      norm: bn\n",
            "rank: 0\n",
            "distributed: False\n",
            "mp: False\n",
            "task_name: shapenetpart_pix4point\n",
            "cfg_basename: pix4point\n",
            "opts: True---val_freq-1---epochs-1\n",
            "is_training: True\n",
            "run_name: shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg\n",
            "run_dir: log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg\n",
            "exp_dir: log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg\n",
            "log_dir: log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg\n",
            "ckpt_dir: log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg/checkpoint\n",
            "log_path: log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg.log\n",
            "cfg_path: log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg/cfg.yaml\n",
            "data/ShapeNetPart/shapenetcore_partanno_segmentation_benchmark_v0_normal/processed/test_2048_fps.pkl load successfully\n",
            "\u001b[32m[12/07 11:59:07 ShapeNetPartNormal]: \u001b[0mlength of validation dataset: 2874\n",
            "\u001b[32m[12/07 11:59:07 ShapeNetPartNormal]: \u001b[0mnumber of classes of the dataset: None\n",
            "\u001b[32m[12/07 11:59:10 ShapeNetPartNormal]: \u001b[0mBasePartSeg(\n",
            "  (encoder): PointViT(\n",
            "    (patch_embed): P3Embed(\n",
            "      (grouper): KNNGroup(\n",
            "        (knn): KNN()\n",
            "      )\n",
            "      (convs): ModuleList(\n",
            "        (0): ModuleList(\n",
            "          (0): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(10, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): ModuleList(\n",
            "          (0): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(195, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pos_embed): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Linear(in_features=3, out_features=128, bias=True)\n",
            "        (1): GELU(approximate=none)\n",
            "      )\n",
            "      (1): Linear(in_features=128, out_features=384, bias=True)\n",
            "    )\n",
            "    (proj): Identity()\n",
            "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "    (blocks): ModuleList(\n",
            "      (0): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): PointViTPartDecoder(\n",
            "    (convc): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
            "        (1): GELU(approximate=none)\n",
            "      )\n",
            "    )\n",
            "    (decoder): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): FeaturePropogation(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(455, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv1d(384, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): FeaturePropogation(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(576, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv1d(384, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): SegHead(\n",
            "    (head): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv1d(1536, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): Dropout(p=0.5, inplace=False)\n",
            "      (2): Sequential(\n",
            "        (0): Conv1d(256, 50, kernel_size=(1,), stride=(1,))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[12/07 11:59:10 ShapeNetPartNormal]: \u001b[0mNumber of params: 23.8189 M\n",
            "\u001b[32m[12/07 11:59:10 ShapeNetPartNormal]: \u001b[0mParam groups = {\n",
            "  \"decay\": {\n",
            "    \"weight_decay\": 0.0001,\n",
            "    \"params\": [\n",
            "      \"encoder.cls_token\",\n",
            "      \"encoder.cls_pos\",\n",
            "      \"encoder.patch_embed.convs.0.0.0.0.weight\",\n",
            "      \"encoder.patch_embed.convs.0.0.1.0.weight\",\n",
            "      \"encoder.patch_embed.convs.0.1.0.0.weight\",\n",
            "      \"encoder.patch_embed.convs.0.1.1.0.weight\",\n",
            "      \"encoder.patch_embed.convs.1.0.0.0.weight\",\n",
            "      \"encoder.patch_embed.convs.1.0.1.0.weight\",\n",
            "      \"encoder.patch_embed.convs.1.1.0.0.weight\",\n",
            "      \"encoder.patch_embed.convs.1.1.1.0.weight\",\n",
            "      \"encoder.pos_embed.0.0.weight\",\n",
            "      \"encoder.pos_embed.1.weight\",\n",
            "      \"encoder.blocks.0.attn.qkv.weight\",\n",
            "      \"encoder.blocks.0.attn.proj.weight\",\n",
            "      \"encoder.blocks.0.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.0.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.1.attn.qkv.weight\",\n",
            "      \"encoder.blocks.1.attn.proj.weight\",\n",
            "      \"encoder.blocks.1.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.1.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.2.attn.qkv.weight\",\n",
            "      \"encoder.blocks.2.attn.proj.weight\",\n",
            "      \"encoder.blocks.2.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.2.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.3.attn.qkv.weight\",\n",
            "      \"encoder.blocks.3.attn.proj.weight\",\n",
            "      \"encoder.blocks.3.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.3.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.4.attn.qkv.weight\",\n",
            "      \"encoder.blocks.4.attn.proj.weight\",\n",
            "      \"encoder.blocks.4.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.4.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.5.attn.qkv.weight\",\n",
            "      \"encoder.blocks.5.attn.proj.weight\",\n",
            "      \"encoder.blocks.5.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.5.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.6.attn.qkv.weight\",\n",
            "      \"encoder.blocks.6.attn.proj.weight\",\n",
            "      \"encoder.blocks.6.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.6.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.7.attn.qkv.weight\",\n",
            "      \"encoder.blocks.7.attn.proj.weight\",\n",
            "      \"encoder.blocks.7.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.7.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.8.attn.qkv.weight\",\n",
            "      \"encoder.blocks.8.attn.proj.weight\",\n",
            "      \"encoder.blocks.8.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.8.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.9.attn.qkv.weight\",\n",
            "      \"encoder.blocks.9.attn.proj.weight\",\n",
            "      \"encoder.blocks.9.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.9.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.10.attn.qkv.weight\",\n",
            "      \"encoder.blocks.10.attn.proj.weight\",\n",
            "      \"encoder.blocks.10.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.10.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.11.attn.qkv.weight\",\n",
            "      \"encoder.blocks.11.attn.proj.weight\",\n",
            "      \"encoder.blocks.11.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.11.mlp.fc2.weight\",\n",
            "      \"decoder.convc.0.0.weight\",\n",
            "      \"decoder.decoder.0.0.convs.0.0.weight\",\n",
            "      \"decoder.decoder.0.0.convs.1.0.weight\",\n",
            "      \"decoder.decoder.1.0.convs.0.0.weight\",\n",
            "      \"decoder.decoder.1.0.convs.1.0.weight\",\n",
            "      \"head.head.0.0.weight\",\n",
            "      \"head.head.2.0.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 1.0\n",
            "  },\n",
            "  \"no_decay\": {\n",
            "    \"weight_decay\": 0.0,\n",
            "    \"params\": [\n",
            "      \"encoder.patch_embed.convs.0.0.0.1.weight\",\n",
            "      \"encoder.patch_embed.convs.0.0.0.1.bias\",\n",
            "      \"encoder.patch_embed.convs.0.0.1.0.bias\",\n",
            "      \"encoder.patch_embed.convs.0.1.0.1.weight\",\n",
            "      \"encoder.patch_embed.convs.0.1.0.1.bias\",\n",
            "      \"encoder.patch_embed.convs.0.1.1.1.weight\",\n",
            "      \"encoder.patch_embed.convs.0.1.1.1.bias\",\n",
            "      \"encoder.patch_embed.convs.1.0.0.1.weight\",\n",
            "      \"encoder.patch_embed.convs.1.0.0.1.bias\",\n",
            "      \"encoder.patch_embed.convs.1.0.1.0.bias\",\n",
            "      \"encoder.patch_embed.convs.1.1.0.1.weight\",\n",
            "      \"encoder.patch_embed.convs.1.1.0.1.bias\",\n",
            "      \"encoder.patch_embed.convs.1.1.1.1.weight\",\n",
            "      \"encoder.patch_embed.convs.1.1.1.1.bias\",\n",
            "      \"encoder.pos_embed.0.0.bias\",\n",
            "      \"encoder.pos_embed.1.bias\",\n",
            "      \"encoder.blocks.0.norm1.weight\",\n",
            "      \"encoder.blocks.0.norm1.bias\",\n",
            "      \"encoder.blocks.0.attn.qkv.bias\",\n",
            "      \"encoder.blocks.0.attn.proj.bias\",\n",
            "      \"encoder.blocks.0.norm2.weight\",\n",
            "      \"encoder.blocks.0.norm2.bias\",\n",
            "      \"encoder.blocks.0.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.0.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.1.norm1.weight\",\n",
            "      \"encoder.blocks.1.norm1.bias\",\n",
            "      \"encoder.blocks.1.attn.qkv.bias\",\n",
            "      \"encoder.blocks.1.attn.proj.bias\",\n",
            "      \"encoder.blocks.1.norm2.weight\",\n",
            "      \"encoder.blocks.1.norm2.bias\",\n",
            "      \"encoder.blocks.1.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.1.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.2.norm1.weight\",\n",
            "      \"encoder.blocks.2.norm1.bias\",\n",
            "      \"encoder.blocks.2.attn.qkv.bias\",\n",
            "      \"encoder.blocks.2.attn.proj.bias\",\n",
            "      \"encoder.blocks.2.norm2.weight\",\n",
            "      \"encoder.blocks.2.norm2.bias\",\n",
            "      \"encoder.blocks.2.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.2.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.3.norm1.weight\",\n",
            "      \"encoder.blocks.3.norm1.bias\",\n",
            "      \"encoder.blocks.3.attn.qkv.bias\",\n",
            "      \"encoder.blocks.3.attn.proj.bias\",\n",
            "      \"encoder.blocks.3.norm2.weight\",\n",
            "      \"encoder.blocks.3.norm2.bias\",\n",
            "      \"encoder.blocks.3.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.3.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.4.norm1.weight\",\n",
            "      \"encoder.blocks.4.norm1.bias\",\n",
            "      \"encoder.blocks.4.attn.qkv.bias\",\n",
            "      \"encoder.blocks.4.attn.proj.bias\",\n",
            "      \"encoder.blocks.4.norm2.weight\",\n",
            "      \"encoder.blocks.4.norm2.bias\",\n",
            "      \"encoder.blocks.4.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.4.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.5.norm1.weight\",\n",
            "      \"encoder.blocks.5.norm1.bias\",\n",
            "      \"encoder.blocks.5.attn.qkv.bias\",\n",
            "      \"encoder.blocks.5.attn.proj.bias\",\n",
            "      \"encoder.blocks.5.norm2.weight\",\n",
            "      \"encoder.blocks.5.norm2.bias\",\n",
            "      \"encoder.blocks.5.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.5.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.6.norm1.weight\",\n",
            "      \"encoder.blocks.6.norm1.bias\",\n",
            "      \"encoder.blocks.6.attn.qkv.bias\",\n",
            "      \"encoder.blocks.6.attn.proj.bias\",\n",
            "      \"encoder.blocks.6.norm2.weight\",\n",
            "      \"encoder.blocks.6.norm2.bias\",\n",
            "      \"encoder.blocks.6.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.6.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.7.norm1.weight\",\n",
            "      \"encoder.blocks.7.norm1.bias\",\n",
            "      \"encoder.blocks.7.attn.qkv.bias\",\n",
            "      \"encoder.blocks.7.attn.proj.bias\",\n",
            "      \"encoder.blocks.7.norm2.weight\",\n",
            "      \"encoder.blocks.7.norm2.bias\",\n",
            "      \"encoder.blocks.7.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.7.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.8.norm1.weight\",\n",
            "      \"encoder.blocks.8.norm1.bias\",\n",
            "      \"encoder.blocks.8.attn.qkv.bias\",\n",
            "      \"encoder.blocks.8.attn.proj.bias\",\n",
            "      \"encoder.blocks.8.norm2.weight\",\n",
            "      \"encoder.blocks.8.norm2.bias\",\n",
            "      \"encoder.blocks.8.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.8.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.9.norm1.weight\",\n",
            "      \"encoder.blocks.9.norm1.bias\",\n",
            "      \"encoder.blocks.9.attn.qkv.bias\",\n",
            "      \"encoder.blocks.9.attn.proj.bias\",\n",
            "      \"encoder.blocks.9.norm2.weight\",\n",
            "      \"encoder.blocks.9.norm2.bias\",\n",
            "      \"encoder.blocks.9.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.9.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.10.norm1.weight\",\n",
            "      \"encoder.blocks.10.norm1.bias\",\n",
            "      \"encoder.blocks.10.attn.qkv.bias\",\n",
            "      \"encoder.blocks.10.attn.proj.bias\",\n",
            "      \"encoder.blocks.10.norm2.weight\",\n",
            "      \"encoder.blocks.10.norm2.bias\",\n",
            "      \"encoder.blocks.10.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.10.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.11.norm1.weight\",\n",
            "      \"encoder.blocks.11.norm1.bias\",\n",
            "      \"encoder.blocks.11.attn.qkv.bias\",\n",
            "      \"encoder.blocks.11.attn.proj.bias\",\n",
            "      \"encoder.blocks.11.norm2.weight\",\n",
            "      \"encoder.blocks.11.norm2.bias\",\n",
            "      \"encoder.blocks.11.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.11.mlp.fc2.bias\",\n",
            "      \"encoder.norm.weight\",\n",
            "      \"encoder.norm.bias\",\n",
            "      \"decoder.convc.0.0.bias\",\n",
            "      \"decoder.decoder.0.0.convs.0.1.weight\",\n",
            "      \"decoder.decoder.0.0.convs.0.1.bias\",\n",
            "      \"decoder.decoder.0.0.convs.1.1.weight\",\n",
            "      \"decoder.decoder.0.0.convs.1.1.bias\",\n",
            "      \"decoder.decoder.1.0.convs.0.1.weight\",\n",
            "      \"decoder.decoder.1.0.convs.0.1.bias\",\n",
            "      \"decoder.decoder.1.0.convs.1.1.weight\",\n",
            "      \"decoder.decoder.1.0.convs.1.1.bias\",\n",
            "      \"head.head.0.1.weight\",\n",
            "      \"head.head.0.1.bias\",\n",
            "      \"head.head.2.0.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 1.0\n",
            "  }\n",
            "}\n",
            "\u001b[32m[12/07 11:59:10 ShapeNetPartNormal]: \u001b[0mLoad encoder only, finetuning from pretrained/imagenet/mae_s.pth\n",
            "\u001b[32m[12/07 11:59:10 ShapeNetPartNormal]: \u001b[0mmissing_keys\n",
            "\u001b[32m[12/07 11:59:10 ShapeNetPartNormal]: \u001b[0mSome model parameters or buffers are not found in the checkpoint:\n",
            "  \u001b[34mcls_pos\u001b[0m\n",
            "  \u001b[34mpatch_embed.convs.0.0.0.0.weight\u001b[0m\n",
            "  \u001b[34mpatch_embed.convs.0.0.0.1.{weight, bias, running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mpatch_embed.convs.0.0.1.0.{weight, bias}\u001b[0m\n",
            "  \u001b[34mpatch_embed.convs.0.1.0.0.weight\u001b[0m\n",
            "  \u001b[34mpatch_embed.convs.0.1.0.1.{weight, bias, running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mpatch_embed.convs.0.1.1.0.weight\u001b[0m\n",
            "  \u001b[34mpatch_embed.convs.0.1.1.1.{weight, bias, running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mpatch_embed.convs.1.0.0.0.weight\u001b[0m\n",
            "  \u001b[34mpatch_embed.convs.1.0.0.1.{weight, bias, running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mpatch_embed.convs.1.0.1.0.{weight, bias}\u001b[0m\n",
            "  \u001b[34mpatch_embed.convs.1.1.0.0.weight\u001b[0m\n",
            "  \u001b[34mpatch_embed.convs.1.1.0.1.{weight, bias, running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mpatch_embed.convs.1.1.1.0.weight\u001b[0m\n",
            "  \u001b[34mpatch_embed.convs.1.1.1.1.{weight, bias, running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mpos_embed.0.0.{weight, bias}\u001b[0m\n",
            "  \u001b[34mpos_embed.1.{weight, bias}\u001b[0m\n",
            "\u001b[32m[12/07 11:59:10 ShapeNetPartNormal]: \u001b[0munexpected_keys\n",
            "\u001b[32m[12/07 11:59:10 ShapeNetPartNormal]: \u001b[0mThe checkpoint state_dict contains keys that are not used by the model:\n",
            "  \u001b[35mhead.{weight, bias}\u001b[0m\n",
            "  \u001b[35mpatch_embed.proj.{weight, bias}\u001b[0m\n",
            "\u001b[32m[12/07 11:59:10 ShapeNetPartNormal]: \u001b[0mSuccessful Loading the ckpt from pretrained/imagenet/mae_s.pth\n",
            "\u001b[32m[12/07 11:59:10 ShapeNetPartNormal]: \u001b[0mckpts @ -1 epoch( {} )\n",
            "\u001b[32m[12/07 11:59:11 ShapeNetPartNormal]: \u001b[0mlength of training dataset: 13998\n",
            "Train Epoch [1/1] Loss 0.009 : 100% 1749/1749 [08:51<00:00,  3.29it/s]\n",
            "100% 360/360 [00:37<00:00,  9.67it/s]\n",
            "\u001b[32m[12/07 12:08:40 ShapeNetPartNormal]: \u001b[0mTest Epoch [1/1],Instance mIoU 77.46, Class mIoU 63.78, \n",
            " Class mIoUs tensor([74.7530, 44.7161, 80.4390, 58.8928, 88.6854, 55.8953, 83.8422, 80.5936,\n",
            "        77.3763, 93.8097, 25.8863, 48.6142, 56.9818, 23.0868, 49.2220, 77.7445],\n",
            "       device='cuda:0')\n",
            "\u001b[32m[12/07 12:08:40 ShapeNetPartNormal]: \u001b[0mFind a better ckpt @E1, val_ins_miou 77.46 val_cls_miou 63.78, \n",
            "cls_mious: tensor([74.7530, 44.7161, 80.4390, 58.8928, 88.6854, 55.8953, 83.8422, 80.5936,\n",
            "        77.3763, 93.8097, 25.8863, 48.6142, 56.9818, 23.0868, 49.2220, 77.7445],\n",
            "       device='cuda:0')\n",
            "\u001b[32m[12/07 12:08:42 ShapeNetPartNormal]: \u001b[0mFound the best model and saved in log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg/checkpoint/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg_ckpt_best.pth\n",
            "\u001b[32m[12/07 12:08:42 ShapeNetPartNormal]: \u001b[0mBest Epoch 1,Instance mIoU 77.46, Class mIoU 63.78, \n",
            " Class mIoUs tensor([74.7530, 44.7161, 80.4390, 58.8928, 88.6854, 55.8953, 83.8422, 80.5936,\n",
            "        77.3763, 93.8097, 25.8863, 48.6142, 56.9818, 23.0868, 49.2220, 77.7445],\n",
            "       device='cuda:0')\n",
            "\u001b[32m[12/07 12:08:42 ShapeNetPartNormal]: \u001b[0mSuccessful Loading the ckpt from log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg/checkpoint/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg_ckpt_best.pth\n",
            "\u001b[32m[12/07 12:08:42 ShapeNetPartNormal]: \u001b[0mckpts @ 1 epoch( {} )\n",
            "100% 360/360 [06:36<00:00,  1.10s/it]\n",
            "\u001b[32m[12/07 12:15:19 ShapeNetPartNormal]: \u001b[0mTest Epoch [1/1],Instance mIoU 77.86, Class mIoU 63.90, \n",
            " Class mIoUs tensor([75.9013, 44.7161, 80.3188, 58.5227, 89.2597, 55.6434, 83.2524, 80.8190,\n",
            "        78.5015, 94.8214, 25.9276, 48.6142, 57.5942, 23.0827, 47.4743, 77.8808],\n",
            "       device='cuda:0')\n",
            "\u001b[32m[12/07 12:15:19 ShapeNetPartNormal]: \u001b[0m---Voting---\n",
            "Best Epoch 1,Voting Instance mIoU 77.86, Voting Class mIoU 63.90, \n",
            " Voting Class mIoUs tensor([75.9013, 44.7161, 80.3188, 58.5227, 89.2597, 55.6434, 83.2524, 80.8190,\n",
            "        78.5015, 94.8214, 25.9276, 48.6142, 57.5942, 23.0827, 47.4743, 77.8808],\n",
            "       device='cuda:0')\n",
            "Traceback (most recent call last):\n",
            "  File \"examples/shapenetpart/main.py\", line 447, in <module>\n",
            "    main(0, cfg)\n",
            "  File \"examples/shapenetpart/main.py\", line 283, in main\n",
            "    dist.destroy_process_group()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py\", line 797, in destroy_process_group\n",
            "    assert pg is not None\n",
            "AssertionError\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_path = 'log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg/checkpoint/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg_ckpt_best.pth'\n",
        "!python examples/shapenetpart/main.py --cfg cfgs/shapenetpart_pix4point/pix4point.yaml mode=test pretrained_path=\"$pretrained_path\" --wandb.use_wandb True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrxViNARdBU-",
        "outputId": "5d413ddc-1789-4d01-9196-333301ce8734"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "launch mp with 1 GPUs, current rank: 0\n",
            "\u001b[32m[12/07 12:19:40 ShapeNetPartNormal]: \u001b[0mInitializing MLIR with module: _site_initialize_0\n",
            "\u001b[32m[12/07 12:19:40 ShapeNetPartNormal]: \u001b[0mRegistering dialects from initializer <module 'jaxlib.mlir._mlir_libs._site_initialize_0' from '/usr/local/lib/python3.8/dist-packages/jaxlib/mlir/_mlir_libs/_site_initialize_0.so'>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmakartkar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/PointNeXt/wandb/run-20221207_121942-sxy8mssc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mshapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/makartkar/pix4point-ShapeNetPart\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/makartkar/pix4point-ShapeNetPart/runs/sxy8mssc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[32m[12/07 12:19:45 ShapeNetPartNormal]: \u001b[0mdist_url: tcp://localhost:8888\n",
            "dist_backend: nccl\n",
            "multiprocessing_distributed: False\n",
            "ngpus_per_node: 1\n",
            "world_size: 1\n",
            "launcher: mp\n",
            "local_rank: 0\n",
            "use_gpu: True\n",
            "seed: 6313\n",
            "epoch: 0\n",
            "epochs: 300\n",
            "ignore_index: None\n",
            "val_fn: validate\n",
            "deterministic: False\n",
            "sync_bn: False\n",
            "criterion_args:\n",
            "  NAME: Poly1FocalLoss\n",
            "use_mask: False\n",
            "grad_norm_clip: 1\n",
            "layer_decay: 0\n",
            "step_per_update: 1\n",
            "start_epoch: 1\n",
            "sched_on_epoch: True\n",
            "wandb:\n",
            "  use_wandb: True\n",
            "  project: pix4point-ShapeNetPart\n",
            "  tags: ['test']\n",
            "  name: shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg\n",
            "use_amp: False\n",
            "use_voting: False\n",
            "val_freq: 1\n",
            "resume: False\n",
            "test: False\n",
            "finetune: False\n",
            "mode: test\n",
            "logname: None\n",
            "load_path: None\n",
            "print_freq: 10\n",
            "save_freq: -1\n",
            "root_dir: log/shapenetpart_pix4point\n",
            "pretrained_path: log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg/checkpoint/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg_ckpt_best.pth\n",
            "datatransforms:\n",
            "  train: ['PointsToTensor', 'PointCloudScaling', 'PointCloudCenterAndNormalize', 'PointCloudJitter', 'ChromaticDropGPU']\n",
            "  val: ['PointsToTensor', 'PointCloudCenterAndNormalize']\n",
            "  vote: ['PointCloudScaling']\n",
            "  kwargs:\n",
            "    jitter_sigma: 0.001\n",
            "    jitter_clip: 0.005\n",
            "    scale: [0.8, 1.2]\n",
            "    gravity_dim: 1\n",
            "    angle: [0, 1.0, 0]\n",
            "feature_keys: pos,x,heights\n",
            "dataset:\n",
            "  common:\n",
            "    NAME: ShapeNetPartNormal\n",
            "    data_root: data/ShapeNetPart/shapenetcore_partanno_segmentation_benchmark_v0_normal\n",
            "    use_normal: True\n",
            "    num_points: 2048\n",
            "  train:\n",
            "    split: trainval\n",
            "  val:\n",
            "    split: test\n",
            "    presample: True\n",
            "num_classes: 50\n",
            "shape_classes: 16\n",
            "num_points: 2048\n",
            "normal_channel: True\n",
            "batch_size: 8\n",
            "dataloader:\n",
            "  num_workers: 6\n",
            "num_votes: 10\n",
            "refine: True\n",
            "lr: 0.0005\n",
            "optimizer:\n",
            "  NAME: adamw\n",
            "  weight_decay: 0.0001\n",
            "sched: multistep\n",
            "decay_epochs: [210, 270]\n",
            "decay_rate: 0.1\n",
            "warmup_epochs: 0\n",
            "model:\n",
            "  NAME: BasePartSeg\n",
            "  encoder_args:\n",
            "    NAME: PointViT\n",
            "    in_channels: 7\n",
            "    embed_dim: 384\n",
            "    depth: 12\n",
            "    num_heads: 6\n",
            "    mlp_ratio: 4.0\n",
            "    drop_rate: 0.0\n",
            "    attn_drop_rate: 0.0\n",
            "    drop_path_rate: 0.1\n",
            "    add_pos_each_block: True\n",
            "    qkv_bias: True\n",
            "    act_args:\n",
            "      act: gelu\n",
            "    norm_args:\n",
            "      norm: ln\n",
            "      eps: 1e-06\n",
            "    embed_args:\n",
            "      NAME: P3Embed\n",
            "      feature_type: dp_df\n",
            "      reduction: max\n",
            "      sample_ratio: 0.0625\n",
            "      normalize_dp: False\n",
            "      group_size: 32\n",
            "      subsample: fps\n",
            "      group: knn\n",
            "      conv_args:\n",
            "        order: conv-norm-act\n",
            "      layers: 4\n",
            "      norm_args:\n",
            "        norm: bn\n",
            "  decoder_args:\n",
            "    NAME: PointViTPartDecoder\n",
            "    channel_scaling: 1\n",
            "    global_feat: cls,max,avg\n",
            "    progressive_input: True\n",
            "  cls_args:\n",
            "    NAME: SegHead\n",
            "    num_classes: 50\n",
            "    mlps: [256]\n",
            "    in_channels: None\n",
            "    norm_args:\n",
            "      norm: bn\n",
            "rank: 0\n",
            "distributed: False\n",
            "mp: False\n",
            "task_name: shapenetpart_pix4point\n",
            "cfg_basename: pix4point\n",
            "opts: mode=test-True\n",
            "is_training: False\n",
            "run_dir: log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg\n",
            "log_dir: log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg\n",
            "run_name: shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg\n",
            "ckpt_dir: log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg/checkpoint\n",
            "code_dir: log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg/code\n",
            "log_path: log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg20221207-121938-cc7CvzrL4AhrggPLjnhEAT.log\n",
            "cfg_path: log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg/cfg.yaml\n",
            "data/ShapeNetPart/shapenetcore_partanno_segmentation_benchmark_v0_normal/processed/test_2048_fps.pkl load successfully\n",
            "\u001b[32m[12/07 12:19:46 ShapeNetPartNormal]: \u001b[0mlength of validation dataset: 2874\n",
            "\u001b[32m[12/07 12:19:46 ShapeNetPartNormal]: \u001b[0mnumber of classes of the dataset: None\n",
            "\u001b[32m[12/07 12:19:50 ShapeNetPartNormal]: \u001b[0mBasePartSeg(\n",
            "  (encoder): PointViT(\n",
            "    (patch_embed): P3Embed(\n",
            "      (grouper): KNNGroup(\n",
            "        (knn): KNN()\n",
            "      )\n",
            "      (convs): ModuleList(\n",
            "        (0): ModuleList(\n",
            "          (0): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(10, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): ModuleList(\n",
            "          (0): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(195, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "          (1): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pos_embed): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Linear(in_features=3, out_features=128, bias=True)\n",
            "        (1): GELU(approximate=none)\n",
            "      )\n",
            "      (1): Linear(in_features=128, out_features=384, bias=True)\n",
            "    )\n",
            "    (proj): Identity()\n",
            "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "    (blocks): ModuleList(\n",
            "      (0): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): DropPath()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): PointViTPartDecoder(\n",
            "    (convc): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
            "        (1): GELU(approximate=none)\n",
            "      )\n",
            "    )\n",
            "    (decoder): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): FeaturePropogation(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(455, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv1d(384, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): FeaturePropogation(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(576, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv1d(384, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): SegHead(\n",
            "    (head): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv1d(1536, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): Dropout(p=0.5, inplace=False)\n",
            "      (2): Sequential(\n",
            "        (0): Conv1d(256, 50, kernel_size=(1,), stride=(1,))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[12/07 12:19:50 ShapeNetPartNormal]: \u001b[0mNumber of params: 23.8189 M\n",
            "\u001b[32m[12/07 12:19:50 ShapeNetPartNormal]: \u001b[0mParam groups = {\n",
            "  \"decay\": {\n",
            "    \"weight_decay\": 0.0001,\n",
            "    \"params\": [\n",
            "      \"encoder.cls_token\",\n",
            "      \"encoder.cls_pos\",\n",
            "      \"encoder.patch_embed.convs.0.0.0.0.weight\",\n",
            "      \"encoder.patch_embed.convs.0.0.1.0.weight\",\n",
            "      \"encoder.patch_embed.convs.0.1.0.0.weight\",\n",
            "      \"encoder.patch_embed.convs.0.1.1.0.weight\",\n",
            "      \"encoder.patch_embed.convs.1.0.0.0.weight\",\n",
            "      \"encoder.patch_embed.convs.1.0.1.0.weight\",\n",
            "      \"encoder.patch_embed.convs.1.1.0.0.weight\",\n",
            "      \"encoder.patch_embed.convs.1.1.1.0.weight\",\n",
            "      \"encoder.pos_embed.0.0.weight\",\n",
            "      \"encoder.pos_embed.1.weight\",\n",
            "      \"encoder.blocks.0.attn.qkv.weight\",\n",
            "      \"encoder.blocks.0.attn.proj.weight\",\n",
            "      \"encoder.blocks.0.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.0.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.1.attn.qkv.weight\",\n",
            "      \"encoder.blocks.1.attn.proj.weight\",\n",
            "      \"encoder.blocks.1.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.1.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.2.attn.qkv.weight\",\n",
            "      \"encoder.blocks.2.attn.proj.weight\",\n",
            "      \"encoder.blocks.2.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.2.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.3.attn.qkv.weight\",\n",
            "      \"encoder.blocks.3.attn.proj.weight\",\n",
            "      \"encoder.blocks.3.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.3.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.4.attn.qkv.weight\",\n",
            "      \"encoder.blocks.4.attn.proj.weight\",\n",
            "      \"encoder.blocks.4.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.4.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.5.attn.qkv.weight\",\n",
            "      \"encoder.blocks.5.attn.proj.weight\",\n",
            "      \"encoder.blocks.5.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.5.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.6.attn.qkv.weight\",\n",
            "      \"encoder.blocks.6.attn.proj.weight\",\n",
            "      \"encoder.blocks.6.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.6.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.7.attn.qkv.weight\",\n",
            "      \"encoder.blocks.7.attn.proj.weight\",\n",
            "      \"encoder.blocks.7.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.7.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.8.attn.qkv.weight\",\n",
            "      \"encoder.blocks.8.attn.proj.weight\",\n",
            "      \"encoder.blocks.8.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.8.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.9.attn.qkv.weight\",\n",
            "      \"encoder.blocks.9.attn.proj.weight\",\n",
            "      \"encoder.blocks.9.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.9.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.10.attn.qkv.weight\",\n",
            "      \"encoder.blocks.10.attn.proj.weight\",\n",
            "      \"encoder.blocks.10.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.10.mlp.fc2.weight\",\n",
            "      \"encoder.blocks.11.attn.qkv.weight\",\n",
            "      \"encoder.blocks.11.attn.proj.weight\",\n",
            "      \"encoder.blocks.11.mlp.fc1.weight\",\n",
            "      \"encoder.blocks.11.mlp.fc2.weight\",\n",
            "      \"decoder.convc.0.0.weight\",\n",
            "      \"decoder.decoder.0.0.convs.0.0.weight\",\n",
            "      \"decoder.decoder.0.0.convs.1.0.weight\",\n",
            "      \"decoder.decoder.1.0.convs.0.0.weight\",\n",
            "      \"decoder.decoder.1.0.convs.1.0.weight\",\n",
            "      \"head.head.0.0.weight\",\n",
            "      \"head.head.2.0.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 1.0\n",
            "  },\n",
            "  \"no_decay\": {\n",
            "    \"weight_decay\": 0.0,\n",
            "    \"params\": [\n",
            "      \"encoder.patch_embed.convs.0.0.0.1.weight\",\n",
            "      \"encoder.patch_embed.convs.0.0.0.1.bias\",\n",
            "      \"encoder.patch_embed.convs.0.0.1.0.bias\",\n",
            "      \"encoder.patch_embed.convs.0.1.0.1.weight\",\n",
            "      \"encoder.patch_embed.convs.0.1.0.1.bias\",\n",
            "      \"encoder.patch_embed.convs.0.1.1.1.weight\",\n",
            "      \"encoder.patch_embed.convs.0.1.1.1.bias\",\n",
            "      \"encoder.patch_embed.convs.1.0.0.1.weight\",\n",
            "      \"encoder.patch_embed.convs.1.0.0.1.bias\",\n",
            "      \"encoder.patch_embed.convs.1.0.1.0.bias\",\n",
            "      \"encoder.patch_embed.convs.1.1.0.1.weight\",\n",
            "      \"encoder.patch_embed.convs.1.1.0.1.bias\",\n",
            "      \"encoder.patch_embed.convs.1.1.1.1.weight\",\n",
            "      \"encoder.patch_embed.convs.1.1.1.1.bias\",\n",
            "      \"encoder.pos_embed.0.0.bias\",\n",
            "      \"encoder.pos_embed.1.bias\",\n",
            "      \"encoder.blocks.0.norm1.weight\",\n",
            "      \"encoder.blocks.0.norm1.bias\",\n",
            "      \"encoder.blocks.0.attn.qkv.bias\",\n",
            "      \"encoder.blocks.0.attn.proj.bias\",\n",
            "      \"encoder.blocks.0.norm2.weight\",\n",
            "      \"encoder.blocks.0.norm2.bias\",\n",
            "      \"encoder.blocks.0.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.0.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.1.norm1.weight\",\n",
            "      \"encoder.blocks.1.norm1.bias\",\n",
            "      \"encoder.blocks.1.attn.qkv.bias\",\n",
            "      \"encoder.blocks.1.attn.proj.bias\",\n",
            "      \"encoder.blocks.1.norm2.weight\",\n",
            "      \"encoder.blocks.1.norm2.bias\",\n",
            "      \"encoder.blocks.1.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.1.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.2.norm1.weight\",\n",
            "      \"encoder.blocks.2.norm1.bias\",\n",
            "      \"encoder.blocks.2.attn.qkv.bias\",\n",
            "      \"encoder.blocks.2.attn.proj.bias\",\n",
            "      \"encoder.blocks.2.norm2.weight\",\n",
            "      \"encoder.blocks.2.norm2.bias\",\n",
            "      \"encoder.blocks.2.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.2.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.3.norm1.weight\",\n",
            "      \"encoder.blocks.3.norm1.bias\",\n",
            "      \"encoder.blocks.3.attn.qkv.bias\",\n",
            "      \"encoder.blocks.3.attn.proj.bias\",\n",
            "      \"encoder.blocks.3.norm2.weight\",\n",
            "      \"encoder.blocks.3.norm2.bias\",\n",
            "      \"encoder.blocks.3.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.3.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.4.norm1.weight\",\n",
            "      \"encoder.blocks.4.norm1.bias\",\n",
            "      \"encoder.blocks.4.attn.qkv.bias\",\n",
            "      \"encoder.blocks.4.attn.proj.bias\",\n",
            "      \"encoder.blocks.4.norm2.weight\",\n",
            "      \"encoder.blocks.4.norm2.bias\",\n",
            "      \"encoder.blocks.4.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.4.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.5.norm1.weight\",\n",
            "      \"encoder.blocks.5.norm1.bias\",\n",
            "      \"encoder.blocks.5.attn.qkv.bias\",\n",
            "      \"encoder.blocks.5.attn.proj.bias\",\n",
            "      \"encoder.blocks.5.norm2.weight\",\n",
            "      \"encoder.blocks.5.norm2.bias\",\n",
            "      \"encoder.blocks.5.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.5.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.6.norm1.weight\",\n",
            "      \"encoder.blocks.6.norm1.bias\",\n",
            "      \"encoder.blocks.6.attn.qkv.bias\",\n",
            "      \"encoder.blocks.6.attn.proj.bias\",\n",
            "      \"encoder.blocks.6.norm2.weight\",\n",
            "      \"encoder.blocks.6.norm2.bias\",\n",
            "      \"encoder.blocks.6.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.6.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.7.norm1.weight\",\n",
            "      \"encoder.blocks.7.norm1.bias\",\n",
            "      \"encoder.blocks.7.attn.qkv.bias\",\n",
            "      \"encoder.blocks.7.attn.proj.bias\",\n",
            "      \"encoder.blocks.7.norm2.weight\",\n",
            "      \"encoder.blocks.7.norm2.bias\",\n",
            "      \"encoder.blocks.7.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.7.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.8.norm1.weight\",\n",
            "      \"encoder.blocks.8.norm1.bias\",\n",
            "      \"encoder.blocks.8.attn.qkv.bias\",\n",
            "      \"encoder.blocks.8.attn.proj.bias\",\n",
            "      \"encoder.blocks.8.norm2.weight\",\n",
            "      \"encoder.blocks.8.norm2.bias\",\n",
            "      \"encoder.blocks.8.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.8.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.9.norm1.weight\",\n",
            "      \"encoder.blocks.9.norm1.bias\",\n",
            "      \"encoder.blocks.9.attn.qkv.bias\",\n",
            "      \"encoder.blocks.9.attn.proj.bias\",\n",
            "      \"encoder.blocks.9.norm2.weight\",\n",
            "      \"encoder.blocks.9.norm2.bias\",\n",
            "      \"encoder.blocks.9.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.9.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.10.norm1.weight\",\n",
            "      \"encoder.blocks.10.norm1.bias\",\n",
            "      \"encoder.blocks.10.attn.qkv.bias\",\n",
            "      \"encoder.blocks.10.attn.proj.bias\",\n",
            "      \"encoder.blocks.10.norm2.weight\",\n",
            "      \"encoder.blocks.10.norm2.bias\",\n",
            "      \"encoder.blocks.10.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.10.mlp.fc2.bias\",\n",
            "      \"encoder.blocks.11.norm1.weight\",\n",
            "      \"encoder.blocks.11.norm1.bias\",\n",
            "      \"encoder.blocks.11.attn.qkv.bias\",\n",
            "      \"encoder.blocks.11.attn.proj.bias\",\n",
            "      \"encoder.blocks.11.norm2.weight\",\n",
            "      \"encoder.blocks.11.norm2.bias\",\n",
            "      \"encoder.blocks.11.mlp.fc1.bias\",\n",
            "      \"encoder.blocks.11.mlp.fc2.bias\",\n",
            "      \"encoder.norm.weight\",\n",
            "      \"encoder.norm.bias\",\n",
            "      \"decoder.convc.0.0.bias\",\n",
            "      \"decoder.decoder.0.0.convs.0.1.weight\",\n",
            "      \"decoder.decoder.0.0.convs.0.1.bias\",\n",
            "      \"decoder.decoder.0.0.convs.1.1.weight\",\n",
            "      \"decoder.decoder.0.0.convs.1.1.bias\",\n",
            "      \"decoder.decoder.1.0.convs.0.1.weight\",\n",
            "      \"decoder.decoder.1.0.convs.0.1.bias\",\n",
            "      \"decoder.decoder.1.0.convs.1.1.weight\",\n",
            "      \"decoder.decoder.1.0.convs.1.1.bias\",\n",
            "      \"head.head.0.1.weight\",\n",
            "      \"head.head.0.1.bias\",\n",
            "      \"head.head.2.0.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 1.0\n",
            "  }\n",
            "}\n",
            "\u001b[32m[12/07 12:19:50 ShapeNetPartNormal]: \u001b[0mSuccessful Loading the ckpt from log/shapenetpart_pix4point/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg/checkpoint/shapenetpart_pix4point-finetune_encoder-pix4point-ngpus1-seed9494-20221207-115904-NGPx4Lg4hJk3i4B4L9F4Dg_ckpt_best.pth\n",
            "\u001b[32m[12/07 12:19:50 ShapeNetPartNormal]: \u001b[0mckpts @ 1 epoch( {} )\n",
            "100% 360/360 [06:32<00:00,  1.09s/it]\n",
            "\u001b[32m[12/07 12:26:23 ShapeNetPartNormal]: \u001b[0mTest Epoch [0/300],Instance mIoU 77.86, Class mIoU 63.90, \n",
            " Class mIoUs tensor([75.9013, 44.7161, 80.3188, 58.5227, 89.2597, 55.6434, 83.2524, 80.8190,\n",
            "        78.5015, 94.8214, 25.9276, 48.6142, 57.5942, 23.0827, 47.4743, 77.8808],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}